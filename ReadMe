										Tema #1 - ALGORITMI PARALELI SI DISTRIBUITI
									Micro Renderer si Resize folosind tehnici de antialiasing
											Implementare : Lavinia Tache - 332CA

		Antialiasing inseamna inlaturarea componentelor de semnal, care au frecventa mai mare decat este capabil cea 
	care poate fi rezolvata in mod corespunzator de inregistrare.
		Se cere :
			-> implemtarea unui software capabil sa redimensioneze o imagine miscorand pierderea de informatie folosind
	antialiasing de tip super sampling antialiasing.
			-> in a doua parte se cere implementarea unui micro motor de randare, capabil sa contina o poza ce contine o linie.

	Partea |:
			SUPER SAMPLING ANTI ALIASING
		Detaliile despre implemntarea task-urile si algerile facute sunt comentate si explicate in fisierul
	sursa. O sa demonstrez in continuare scalabilitatea si motivul alegerii paralelizarii in acest fel.
		Prin folosirea unei singure directive for(cea mai din exterior bucla) am obtinut:

		Cazul 1 (P5 && resize_factor par):	1 thread:       0.004777
											                  2 thread uri:   0.002630
                  											4 thread uri:   0.001487
                  											8 thread uri:   0.000801

		Cazul 1 (P5 && resize_factor 3):	1 thread:       0.004598
                											2 thread uri:   0.002504
                											4 thread uri:   0.001456
                											8 thread uri:   0.000997

		Cazul 2 (P6 && resize_factor par):	1 thread:       0.010555
                  											2 thread uri:   0.005781
                  											4 thread uri:   0.003078
                  											8 thread uri:   0.001850

		Cazul 2 (P6 && resize_factor 3):	1 thread:       0.010014
                											2 thread uri:   0.005356
                											4 thread uri:   0.002591
                											8 thread uri:   0.001759

		In acest moment for-ul se paralelizeaza default, dar acest lucru ar putea fi modificat prin 
	shedule(type:[chunk_size]), type care poare fi dynamic(se dau iteratii la threaduri in functie de cum se termina), static(stabilit de la inceput) sau guided(se dau iteratii la threaduri in functie de cum se termina ]micsorand in timp marimea chuckzeului).
		Rezultatele obtinute folosind schedule(static):

		Cazul 1 (P5 && resize_factor par):	1 thread:       0.004466
                    										2 thread uri:   0.002687
                    										4 thread uri:   0.001433
                    										8 thread uri:   0.001023

		Cazul 1 (P5 && resize_factor 3):	1 thread:       0.004562
                											2 thread uri:   0.002525
                											4 thread uri:   0.001526
                											8 thread uri:   0.001027

		Cazul 2 (P6 && resize_factor par):	1 thread:       0.010643
                  											2 thread uri:   0.005600
                  											4 thread uri:   0.003013
                  											8 thread uri:   0.001609

		Cazul 2 (P6 && resize_factor 3):	1 thread:       0.009921
                											2 thread uri:   0.005246
                											4 thread uri:   0.002882
                											8 thread uri:   0.001799

		In acest moment, observ ca un schedule cu un chuck static nu aduce o imbunatatire semnificativa. 
	O impartire dinamica duce la:

		Cazul 1 (P5 && resize_factor par):	1 thread:       0.002363
                  											2 thread uri:   0.001329
                  											4 thread uri:   0.000791
                  											8 thread uri:   0.000560

		Cazul 1 (P5 && resize_factor 3):	1 thread:       0.002313
                											2 thread uri:   0.001267
                											4 thread uri:   0.000751
                											8 thread uri:   0.000524

		Cazul 2 (P6 && resize_factor par):	1 thread:       0.005605
                  											2 thread uri:   0.003067
                  											4 thread uri:   0.001725
                  											8 thread uri:   0.001120

		Cazul 2 (P6 && resize_factor 3):	1 thread:       0.005247
                											2 thread uri:   0.002759
                											4 thread uri:   0.001611
                											8 thread uri:   0.001049

		Pentru ca am paralelizat doar for-ul din exterior, am incercat - folosind collapse -  sa paralelizez
	for-urile imbricate. Rezultatele sunt:

		Cazul 1 (P5 && resize_factor par):	1 thread:       0.009040
                    										2 thread uri:   0.008890
                    										4 thread uri:   0.005794
                    										8 thread uri:   0.005198

		Cazul 1 (P5 && resize_factor 3):	1 thread:       0.006910
                											2 thread uri:   0.005151
                											4 thread uri:   0.003147
                											8 thread uri:   0.002622

		Cazul 2 (P6 && resize_factor par):	1 thread:       0.015197
                  											2 thread uri:   0.011290
                  											4 thread uri:   0.006804
                  											8 thread uri:   0.005181

		Cazul 2 (P6 && resize_factor 3):	1 thread:       0.012231
                											2 thread uri:   0.007827
                											4 thread uri:   0.003917
                											8 thread uri:   0.002965

		Pentru calcul sumei am folosit pentru una dintre variante reduction, desi aceasta varianta nu scaleaza
	pentru aceasta varianta de implementare(deoarece - retinut la collapse - un singur thread reuseste doar sa produce overhead):

		Cazul 1 (P5 && resize_factor par):	1 thread:       0.005880
                  											2 thread uri:   0.002626
                  											4 thread uri:   0.001566
                  											8 thread uri:   0.001048

		Cazul 1 (P5 && resize_factor 3):	1 thread:       0.004437
                											2 thread uri:   0.002470
                											4 thread uri:   0.001163
                											8 thread uri:   0.000961

		Cazul 2 (P6 && resize_factor par):	1 thread:       0.010989
                  											2 thread uri:   0.005786
                  											4 thread uri:   0.002698
                  											8 thread uri:   0.001345

		Cazul 2 (P6 && resize_factor 3):	1 thread:       0.009431
                											2 thread uri:   0.005158
                											4 thread uri:   0.002363
                											8 thread uri:   0.001411

	Observatii:
		Voi paraleliza folosind for-ul cel mai din exterior impreuna un chuck calculat dinamic. Reduction cu
	scaleaza pentru ca va ajunge in acel punct cu un singur thread si va produce overhead. 
		Cu privire la collapse (este o varianta care ar trebui sa aduca o imbunatatire semnificativa), insa,
	prin alocarea dinamica a matricei, for-urile paralelizare vor produce multe miss-uri in cache(in cauza
	dimensiunii mari). Daca am aloca incepand cu o anuminte adresa o linie continua din matrice, rata de hit ar
	creste, iar collapse ar scala.



	Partea a || a : 
			MICRO RENDERER
		Un renderer este un software care coverteste elementele matematice 2 sau 3 dimensionale (linii, cuburi, sfere) 	in pixeli ai unei imagini care le reprezinta. Pentru a putea desena linia de 3m grosime din cerinta voi verifica fiecare
	pixel din matricea de pixeli, iar daca distanta de la aceasta pana la ecutia care rerezinta dreapta este mai mica decat 3 voi colora acel pixel negru. In caz contrar ramane alb din initializare. Cateva comentarii asupra implentarii am 	adaugat in fisierul sursa.
		In privinta paralelizarii, pentru a alege cea mai potrivita directiva, am incercat si testat diverse variante(variante ale caror timp de executie va fi prezentat in continuare).

			Pentru folosirea unui singure directive for ( caz in care se paralelizeaza doar cel mai din exterior for) am obtinut:
	1 thread    :  1.467893
	2 thread uri:  0.743023
	4 thread uri:  0.364123
	8 thread uri:  0.187766

			Tind cont de faptul ca #pragma omp for paralelizeaza doar for-ul cel mai din exterior, voi folosi collapse pentru a praleliza for-urile imbricate. Rezultatele in urma folositii collapse(2):
	1 thread    :  1.458539
	2 thread uri:  0.755044
	4 thread uri:  0.375173
	8 thread uri:  0.190885

			In acest moment for-ul se paralelizeaza default, dar acest lucru ar putea fi modificat prin
	shedule(type:[chunk_size]), type care poare fi dynamic(se dau iteratii la threaduri in functie de cum se termina), static(stabilit de la inceput) sau guided(se dau iteratii la threaduri in functie de cum se termina ]micsorand in timp marimea chuckzeului).
		Rezultatele obtinute folosind schedule(static, chuck):

	1 thread    :  1.457603
	2 thread uri:  0.754875
	4 thread uri:  0.374460
	8 thread uri:  0.189971

		Rezultatele obtinute folosind schedule(dynamic):
	1 thread    : 1.455501
	2 thread uri: 0.729731
	4 thread uri: 0.364747
	8 thread uri: 0.182484

		Am observat ca pe aceasta implementare, desi schedule este o alternativa utila pentru a paraleliza un 
	for, ofera rezultatele putin mai optime decat paralelizarea ambelor for-uri imbricate. Dynamic este recomandat pentru situatii in care nu stiu exact "arata" exercutiasi cum ar trebui sa il impartim static ce la inceput. 
		Pentru implementarea aleasa nu exista sectiuni critice sau conficte de scriere/citire din memorie.
